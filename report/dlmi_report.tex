% This version of CVPR template is provided by Ming-Ming Cheng.
% Please leave an issue if you found a bug:
% https://github.com/MCG-NKU/CVPR_Template.

\documentclass[final]{cvpr}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[numbers]{natbib}
\usepackage{notoccite}
\usepackage{subcaption}
\captionsetup{compatibility=false}
\usepackage{graphicx}
\usepackage{fancyvrb}

\setlength{\abovecaptionskip}{-3pt plus 3pt minus 2pt}


% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}


\def\cvprPaperID{34} % *** Enter the CVPR Paper ID here
\def\confYear{CVPR 2021}
%\setcounter{page}{4321} % For final version only


\begin{document}
	
	%%%%%%%%% TITLE
	\title{Classification of Lymphocytosis from Blood Cells\\
		\vspace{1mm}
		\large \normalfont Deep Learning for Medical Imaging}
	
	\author{\textbf{Clément Grisi}\\
		École des Ponts ParisTech\\
		\small \url{grisi.clement@gmail.com}
	\and
	\textbf{Marius Schmidt-Mengin}\\
	École des Ponts ParisTech\\
	\small \url{marius.schmidt.mengin@gmail.com}
	}
	
	\maketitle
	
	\begin{abstract}
		In this report, we emphasize our work for the data challenge organized as part of the Deep Learning for Medical Imaging class. The goal of the challenge is to classify lymphocytosis from blood cells. Through iterations in model definition and data representation, we managed to get $0.96623$ classification accuracy on the academic leaderboard (rank 1).
	\end{abstract}
	
	\vspace{-3mm}
	
	\section{Introduction}
	
	Lymphocytosis is an increase in the number or proportion of lymphocytes in the blood. It's a common finding, which can be either a reaction to infection or acute stress (reactive), or the manifestation of a lymphoproliferative disorder -- a type of cancer of the lymphocytes (tumoral). In clinical practice, the diagnosis as either reactive or tumoral is performed by trained pathologists who visually inspect blood cells under a microscope. The final decision also takes into consideration clinical attributes such as age, gender, and lymphocyte count. In spite of being relatively fast and affordable, lymphocytosis diagnisis lacks reproducibility between experts. Additional clinical tests are often required to confirm the malignant nature of the lymphocytes. However, this analysis is relatively expensive and time-consuming, and therefore is not performed for every patient in practice. In this context, automatic classification has the potential to provide accurate and reproducible diagnosis, saving precious time and ressources by quickly identifying which patient should be referred for flow cytometry analysis.
	
	\section{Problem Definiton}
	
	\subsection{Dataset}
	
	Blood smears and patient attributes were collected from $204$ patients from the routine hematology laboratory of the Lyon SudUniversity Hospital. All included patients have a lymphocyte count above $4\times10^9/L$. The blood smears were automatically produced by a Sysmex automat tool, and the nucleated cells were automatically photographed with a DM-96 device. The training set consists of $142$ patients ($44$ reactive and $98$ malignant cases), and the testing set of $42$ patients. For each patient, we have access to dozens of images of lymphocytes, as well as the following clinical attributes: gender, age, and lymphocyte count.
	
	\subsection{Evalutation Metric} 
	
	This challenge is evaluated on balanced accuracy (BA), which normalizes true positive (TP) and true negative (TN) predictions by the number of positive and negative samples, respectively. In particular, if one denotes false positives as FP and false negatives as FN, we have:
	
	\vspace{-2mm}
	
	\begin{equation*}
		\begin{aligned}
			\text{Sensitivity} & = \dfrac{\text{TP}}{\text{TP}+\text{FN}} ,\quad 
			\text{Specificity} & = \dfrac{\text{TN}}{\text{TN}+\text{FP}}\\
		\end{aligned}
	\end{equation*}

	\begin{equation*}
		\begin{aligned}
			\text{BA} = \dfrac{\text{Sensitivity}+\text{Specificity}}{2}
		\end{aligned}
	\end{equation*}
		
	\vspace{2mm}
	
	\subsection{Weak Supervision through Multiple Instance Learning} 
	
	The idea of weakly supervised learning is to exploit coarse-grained annotations to automatically infer finer-grained information. Coarse-grained information is often readily available in the form of patient level labels, but finer grained annotations are more difficult to obtain. Without precise local annotations, classification models cannot be trained in a fully supervised manner. Therefore, various weakly supervised techniques have recently been developed to overcome this issue. One of these techniques relies on multiple instance learning (MIL), an existing framework largely used in classic computer vision that has recently showed state-of-the-art results in several medical imaging tasks \cite{hou_MIL}. \\
	\\
	Babenko \cite{mil} gives a good example to understand multiple instance learning. Imagine several people, each of them having a key chain that contains a few keys. Some of these people are able to enter a certain room, and some aren’t. The task is to predict whether or not a given key chain can open the door of that room. To solve this problem, one needs to find the exact key that is common to all the \textit{positive} key chains. If one can correctly identify this key, one can also correctly classify an entire key chain - \textit{positive} if it contains the required key, or \textit{negative} if it doesn't.\\
	\\
	Hence, the multiple instance learning framework allows the training of a classifier from weakly labeled data: instead of providing input-label pairs, labels $\ell_b$ are assigned to \emph{sets} or \emph{bags} of instances. In this setting, the true instance labels $\ell_i$ can be thought of as latent variables, as they are not known during training. In our case, we can assume that only a subset of the images available for a patient with tumoral lymphocytosis do carry the information necessary to correctly classify that patient. Identifying these special instances perfectly fits in the multiple instance learning framework.

	\section{Methodology}
	\label{sec:method}
	
	\subsection{Standard MIL Assumption}
	\label{standardMIL}
	
	Under the standard MIL assumption, \emph{positive} patients must contain at least one instance classified as \emph{positive}, while \emph{negative} patients, instead, must have all their instances classified as \emph{negative}.\\
	\\
	The full pipeline is described on Figure \ref{fig:mil_pipeline}. Each forward + backward pass can be broken down into two consecutive stages: 
	
	\begin{itemize}
		\setlength\itemsep{-.0em}
		\item the \emph{inference} phase: during \emph{inference}, the models' weights are frozen. Each instance is run through the network and assigned a number between $0$ and $1$: the more likely an instance to indicate tumoral lymphocytosis, the closer the number assigned to $1$. Once all the instances of a patient have been processed, they can be ranked according to their probability of being \emph{positive}. For each patient, the top-$1$ instance is selected for the second stage: the \emph{learning} phase.
		\item the \emph{learning} phase: we now allow the models' weights to be updated. Given the standard MIL assumption detailed above, one can expect the top-$1$ instance of a \emph{positive} patient to be \emph{positive}, and the top-$1$ instance of a \emph{negative} patient to be \emph{negative}. Hence, as we run the top-$1$ instance through the network, we assign them the label of the patient they come from and update the weights accordingly: the network adjusts its weights in order to assign high probabilities to the top-$1$ instances coming from positive patients, while keeping low probabilities for the top-$1$ instances in negative patients. 
	\end{itemize}

	\begin{figure*}[h!]
		\begin{center}
			\includegraphics[width=0.99\linewidth, trim=0cm 5cm 0cm 0cm, clip]{fig/mil_pipeline.pdf}
		\end{center}
		\caption{MIL Pipeline}
		\label{fig:mil_pipeline}
	\end{figure*}

	\noindent
	 At this stage, you might be wondering ``\textit{how this model converges} ?". It is true that it is not obvious as the model is initialized with ImageNet pre-trained weights, hence there's not reason to believe the model will assign high probabilities to the true positive instances, and low probabilities to true negative instances. And actually, the model does start by assigning random probabilities to each instances!  \\
	 \\
	 To understand how the model gradually learns to discriminate positive instances from negative ones, we need to focus on negative patients, who play a key role in the early stages of training. Under the standard MIL assumption , we know these patients must have all their instances classified as negative. At the end of the first \textit{inference} phase, the model has assigned these instances random probabilities, which can be either high or low. Hence, the top-$1$ instance for negative patient is random. I claim that this doesn't matter. Let me explain you why. What's important is what happens during the first \textit{learning} phase: for each negative patient, we have a (random) instance, which is negative. The model must adjust its weights to assign lower probabilities to each negative instances, such that at the following \textit{inference} step, all instances that look similar to negative instances will be assigned lower probabilities. As a result, all negative instances start to be assigned low probabilities, and positive instances will srand out, with higher probabilities. At this point, the top-$1$ instance of a positive patient is more likely to be a true positive. And the model will adjust its weights to assign higher probabilities to this type of instance.\\
	 \\
	 This simplified reasoning allows to understand	how the model gently learns to discriminate suspicious instances from non-suspicious ones, and automatically infers fine-grained information from coarse-grained labels.\\
	 \\
	The standard MIL assumption implies that a patient is classified positive as soon as the model identifies a suspicious instance in its corresponding bag of instances. During testing, if the top-$1$ instance probability is above the classification threshold $\tau_{class}$, the whole bag is classified positive, otherwise it's classified negative. Letting the top-$1$ instance decide for the whole bag might be too sensitive. Immagine there exists different degrees of severity among positive instances. Looking only at the top-$1$ instance will correctly identify the most severe positive instances, but miss the less severe ones, which might be just as important for the final classification. 
	
	\subsection{Extending  the Scope of the Model} 
	\label{extendedMIL}
	
	In order to deal with the limitation highlighted in the previous section, wcame up with two different idead:
	
	\begin{itemize}
		\item extend the standard MIL assumption by introducing of a hyperparameter $k_{\text{train}}$ which forces the model to look for at least $k_{\text{train}}$ positive instances in positive bags, instead of only $1$
		\item still train with top-$k = 1$, but introduce a hyperparameter $k_{\text{agg}}$ to control the number of instances to aggregate for the final prediction.
	\end{itemize}
	
	\noindent
	Both ideas extend the scope of the model as we no longer let one instance decide for the label assigned to a patient. When taking more instances into account, one need to come up with a strategy to aggregate their probabilities into a single patient-level probability. As we pointed out in the previous section, applying a \textit{max} over instances’ probabilities has the adervse effect of letting the top instance decide for the whole bag. Instead, we might want to consider taking the \textit{mean} probability of the top-$k$ instances. Indeed, there are reason to think that $n$ consecutive instances with high probabilities is a different biomarker than $1$ instance with high probability, followed by $n-1$ instances with probabilities close to zero. Taking the mean probability would allow to distinguish these two cases.\\
	\\
	The experiments conducted tend to confirm that the performances of our model were sensitive to the value of both $k_{\text{train}}$ and $k_{\text{agg}}$ parameters. Using a fixed top-$k$, however, does not generalize well, since different patients have different numbers of instances, hence different numbers of suspicious instances. In order to account for this, one can introduce percentage-based $k_{\text{train}}$ - $k_{\text{agg}}$ values by looking at $$ \max \left(m, \lceil k\% \text{ of instances}\rceil\right) $$ where $m$ is a new hyperparameter that controls the minimum number of instances to aggregate for the final prediction. Given the limited time of the Kaggle challenge, we didn't implent this idea, but encourage the interested reader to do so.
	
	\subsection{Taking More Instances into Consideration}
	\label{topbottom}
	
	According to \cite{owkin}, bottom-$k$ instances are the regions “\textit{which best support the absence of the class}” and could be just as useful as top-$k$ instances to determine the patient label. Indeed, we can expect the bottom-$k$ instances of a positive patient to have a different morphological aspsect than that of a negative patient, which could help refining the lymphocytosis diagnosis. Based on these findings, we decided to take the bottom-$k$ probabilities into account as well when deriving a patient's probability, computing the average probability on the $k_{\text{agg}}$ top instances plus the $k_{\text{agg}}$ bottom intances.\\
	\\
	Results suggest bottom-$k$ instances are just as useful as top-$k$ instances to determine the patient label. Hence, there are reasons to believe that taking all instance into consideration and letting the model identify useful instances for classification could lead to improved results. This is precisely what we investigate in the following section.
	
	\subsection{Taking All Instances into Consideration}
	\label{sec:custom}
	
	Instead of only aggregating the model's output on a subset of instances into a single patient-level quantity, we design a custom aggreagtion function that takes all instances into consideration.\\
	\\
	Let us denote all the predictions (logits) for patient $j$ by $\ell_i^j$, $\: i=1 \dots n_j$.
	To aggregate these predictions, we seek to define some weights $\alpha_i^j \geq 0$ such that we can compute the final prediction logit for patient $j$ by
	$$\ell^j = \sum_{i=1}^{n_j} \alpha_i^j \ell_i^j$$
	(we assume that $\sum_i \alpha_i^j = 1$).
	The confidence score is then given by applying the sigmoid function: $y_j = \sigma(l^j)$.
	Two very common aggregation functions are the arithmetic mean and the maximum, where we would respectively set $\alpha_i^j = 1/n_j$ and 
	
	$$
	\alpha_i^j  = \left\{
	\begin{array}{ll}
		1 \text{ if } \ell_i^j = \max_k \ell_k^j  \\
		0 \text{ otherwise}
	\end{array}
	\right.
	$$
	
	\vspace{1mm}
	\noindent
	We believe that these two aggregation function suffer from different problems. Let us express the gradient of the cross-entropy loss with respect to the logits $\ell_i^j$:
	$$\frac{\partial L_{\text{CE}}}{\partial \ell_i^j} = \alpha_i^j \left(y^j - \bar{y}^j\right)$$
	The maximum function results in sparse gradients (only one $\alpha_i^j$ is nonzero), as only the image that effectively generated the maximum produces a nonzero gradient. On the other hand, the mean function generates equal gradients for all images (all $\alpha_i^j$ are equal to $1/n_j$), meaning that for a positive patients, the "pull" generated by the gradient on negative images (if there are any) is equal to the pull applied to positive images. We developped an aggregation function that intuitively overcomes this problem.
	The idea behind it is similar to focal loss \cite{focal}, which scales the weight so as to put more gradient on predictions that are deemed important in the context of a heavy class imbalance.
	Our aggregation function defines the weights $\alpha_i^j$ by batch normalization \cite{bn} followed by the sigmoid function:
	$$ \alpha_i^j = \sigma\left(\beta + \gamma \frac{\widetilde{\ell_i^j} - \mu}{\sqrt{V+\epsilon}}\right) $$
	where $\; \widetilde{\text{ }} \;$ denotes the stop gradient operation (as in focal loss, no gradients are propagated through the weights), $\gamma$ and $\beta$ are trainable parameters, $\epsilon$ is a small constant and $\mu$ and $V$ are respectively the mean and variance of the logits $\ell_i^j$ over a batch of patients:
	$$\mu = \tfrac 1 {n_1+\dots+n_B} \sum_{j=1}^{B} \sum_{i=1}^{n_j} \ell_i^j$$
	$$V = \tfrac 1 {n_1+\dots+n_B} \sum_{j=1}^{B} \sum_{i=1}^{n_j} \left(\ell_i^j - \mu \right)^2$$
	where $B$ is the batch size. Intuitively, $\alpha_i^j$ is small when $\ell_i^j$ is small compared to the rest of the batch, i.e, when it is likely to be a negative sample.
	
	\begin{table}[h]
		\renewcommand\tablename{Pseudo-code}
		\begin{Verbatim}[fontsize=\footnotesize, samepage=true, frame=single]
			logits = conv(features) # 1 channel
			weights = bn(logits.detach())
			weighted_logits = weights * logits
			y = sigmoid(weighted_logits.sum()/weights.sum())
		\end{Verbatim}
		\caption{PyTorch-like pseudo-code for our custom aggregation function}
		\label{alg2}
	\end{table}
	
	\begin{figure}[h!]
		\begin{center}
			\includegraphics[width=0.95\linewidth]{fig/aggreg_2.pdf}
		\end{center}
		\caption{The distribution of the batch normalized logits over a bag is shown in blue. The red sigmoid shows the weight that is associated to each logit. The smaller the logit, the smaller the weight.}
		\label{fig:aggreg}
	\end{figure}

	\section{Results}
	
	We start with the simple model that only looks at the top-$1$ instance, then we evaluate how the different ideas successively described in Section \ref{sec:method} impact our performances. All our methods share a common structure: we adopt a simple and explainable architecture composed of a ResNet \cite{resnet} backbone followed by an aggregation module. The ablation study is solely conducted for the custom aggreagtion module we designed, as it ended up outperforming all other approaches by a significant marging.\\
	\\
	To compare the different methods fairly, we always run $10$ different trainings. Each training has its own seed for initializing the model and splitting the dataset ($50/50$ split). The $10$ seeds are kept the same for each configuration. Each model is trained for $40$ epochs with a batch size of $16$, resulting in $5$ batches per epoch. Evaluation if performed at the end of every epoch. We always optimize with Adam and a learning rate of $10^{-4}$. When we report a metric for any configuration, we average the $10$ best values of each training, and again average this over all $10$ trainings (the average is thus taken over $100$ values). We always report $95\%$ confidence intervals.

	\subsection{Augmentations}
	
	As the images are centered on a lymphocytes, we always apply a center crop of size $112$ to all images (training, validation and testing). Initial experiments showed that this significantly improves the accuracy and reduces the computational resources. We use vertical and horizontal random flipping. We tried to use more agressive augmentations such as affine transforms and color jittering but this heavily impaired the accuracy.

	\subsection{Backbone Choice}
	
	We compared three backbones: ResNet$18$, ResNet$34$ and ResNet$50$ \cite{resnet}. We also tried a more recent backbone, EfficientNet \cite{efn}, but did not obtain good results. Results for the custom aggregation are summarized in Figure \ref{fig:resnet}. ResNet$18$ achieves the best validation balanced accuracy. This is also what we observe for other aggregation functions. We use ResNet$18$ for all our subsequent experiments.
	
	\begin{figure}[h]
		\begin{center}
			\includegraphics[width=0.47\linewidth]{fig/resnets_bal_acc.pdf}
			\includegraphics[width=0.47\linewidth]{fig/resnets_val_loss.pdf}
		\end{center}
		\caption{Impact of ResNet depth on validation loss and balanced accuracy. The black bars indicate $95\%$ confidence intervals. Shallower seems to be better but the results are not very significative.}
		\label{fig:resnet}
	\end{figure}

	\subsection{Working on a Subset of Instances}
	
	add top-$1$, top-$k$, top-$k$ and bottom-$k$ results
	
	\newpage
	
	\subsection{Working on a All Instances}
	
	We start from a simple baseline and we evaluate different modifications.
	For each configuration, we run $10$ trainings. Each training has its own seed for initializing the model and splitting the dataset ($50/50$ split). The $10$ seeds are kept the same for each configuration. Each model is trained for $40$ epochs with a batch size of $16$, resulting in $5$ batches per epoch. Evaluation if performed at the end of every epoch. We always optimize with Adam and a learning rate of $10^{-4}$. When we report a metric for any configuration, we average the $10$ best validation values obtained during each training, and again average this over all $10$ trainings (the average is thus taken over $100$ values). The reported values thus slightly underestimate the best performance of the network, obtained by taking the top-1 value of each training instead of the top-10. We always report $95\%$ confidence intervals over the 100 values recorded during this process.
	We now describe the practical implementation of the custom aggregation module discussed in Section \ref{sec:custom}. An overview of the pipeline is given in Figure \ref{fig:overview}.
	
	\vspace{2mm}
	
	\begin{figure}[h!]
		\begin{center}
			\includegraphics[width=0.9\linewidth]{fig/graphic3.pdf}
		\end{center}
		\caption{Custom Aggregation Pipeline}
		\label{fig:overview}
	\end{figure}
	
	\noindent
	For a given batch of patients, we take all their images and stack them into a batch.
	This batch is passed independently the backbone, without global pooling. 
	A $1\times 1$ convolution with one output channel is then applied to the resulting feature maps to obtain pixelwise prediction logits.
	These logits are rearranged into bags of scores, each bag corresponding to all  prediction for one patient.
	Finally, the aggregation module transforms each bag of scores into a final patient prediction.\\
	\\
	It is common that the features are aggregated before being linearly mapped to the final prediction score.
	In contrast, we apply the aggregation after reducing each feature to a single prediction logit. We did not find this to impact the results (and in the case where the aggregation function is linear, both methods are equivalent). Doing so makes the model more interpretable as each pixel (in the feature space) of each image can be given a score. \\
	\\
	As the number of images passed through the backbone is often large, we keep the gradients for only a random subset of these images. Pseudo-code for this is provided by Algorithm 1.
	
	
	\begin{table}[h]
		\renewcommand\tablename{Pseudo-code}
		\begin{Verbatim}[fontsize=\footnotesize, samepage=true, frame=single]
			B, 3, H, W = image_batch.shape
			max_forward_size = 512
			max_backward_size = 512
			features = []
			i = 0
			# forward images without keeping gradients
			with torch.no_grad():
			while i < B - max_backward_size:
			j = min(i+max_forward_size,
			num_images-max_backward_size)
			features.append(backbone(images[i:j]))
			i = j
			# forward image with gradients
			features.append(backbone(images[i:]))
			# Note: the images are supposed to be shuffled
			# so that the subset for which the gradients
			# are kept is random.
		\end{Verbatim}
		\caption{PyTorch-like pseudo-code for propagating the gradients through only a subset of the images, in order to not run out of GPU memory.}
		\label{alg1}
	\end{table}
	
	\noindent
	\textbf{Number of images}\\	
	In the upcoming experiment, we  no longer all the images of each patient and aggregate their logits, but only use a small random subset of image per patient during each training step. 	We posit that selecting only a small subset of images provides some regularization to the training process. Indeed, assume that for a given positive patient, one image is easily identified by the network as positive. In this case, the network can ignore the other images. By selecting only a small random subset of images, we avoid this problem, as the easy positive image has a high probability of not being selected. Note that we implicitely make the assumption that in the case of a positive patient, a large number of images contains positive information. This idea is similar to hard negative mining, and could be called ``easy positive avoidance".\\
	\\
	Our wining submission was obtained with this strategy but Figure \ref{fig:num_img} shows that it actually worsened the results when evaluated over 10 runs \footnote{When training with 10 images per patient per batch, we trained for 120 epochs instead of 40, but performed evaluation only every third epoch (so that the number of evaluations is kept the same).}.
	
	\begin{figure}[t]
		\begin{center}
			\includegraphics[width=0.95\linewidth]{fig/num_img_bal_acc.pdf}
		\end{center}
		\caption{The performance when training with subsets of 10 images was generally worse.}
		\label{fig:num_img}
	\end{figure}
	
	\section{Explainability}
	
	Multiple instance learning is very handy as it provides different ways to visualize what the model learned. This is of an inestimable value as deep learning models are often criticized for their lack of explainability. They are frequently referred to as \textit{black boxes} in the sense that one can’t really inspect how they end up predicting one thing instead of another. Summarizing what is discussed in Sections \ref{standardMIL} to \ref{topbottom}, the final classification of a patient is the result of several consecutive steps:
	
	\begin{itemize}
		\itemsep-0.1em
		\item an inference step mapping each input instance to a probability
		\item a ranking step sorting instances by decreasing probability
		\item a learning step comparing the $k_{\text{train}}$ instances probabilities patient labels to update the model’s weights
		\item an aggregation step combining $k_{\text{agg}}$ probabilities into a single prediction
	\end{itemize}
	
	\noindent
	Keeping track of instances probabilities allows deep inspection of the workings of our models. Indeed, comparing instances our model identfies positive with those it identifies negative can reveal which visual patterns the model uses to discriminate between reactive and tumoral lymphocytosis. For lack of time, we didn't conduct this analysis.
	
	\begin{figure}[t]
		\begin{center}
			\includegraphics[width=0.7\linewidth]{fig/explainable1}
		\end{center}
		\caption{Predictions and images for 16 patients from the validation set. The patients where not cherry-picked. For each patient, a red left border indicates a positive ground truth label while green indicates a negative one. For each lymphocyte image, we provide on the left the $4\times 4$ predictions of the network. White means a score of one and black a score of 0.}
		\label{fig:explaina}
	\end{figure}
	
	\section{Conclusion}
	Future work: dilated ResNet ?\\
	Future work: aggregate embeddings using a recurrent neural network / self-attention encoder?
	
	{\small
		\bibliographystyle{unsrt}
		\bibliography{egbib}
	}

\end{document}

%	\begin{figure}[h!]
%	\begin{center}
%		\includegraphics[width=0.47\linewidth]{fig/agg_bal_acc.pdf}
%		\includegraphics[width=0.47\linewidth]{fig/agg_val_loss.pdf}
%	\end{center}
%	\caption{Impact of our BN aggregation function on validation loss and balanced accuracy.  The black bars indicate $95\%$ confidence intervals.}
%	\label{fig:resnet}
%\end{figure}